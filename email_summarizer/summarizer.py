# ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ 

import re
import numpy as np
from typing import List, Tuple, Dict
from collections import Counter
import math
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline

# ëª¨ë¸ ë¡œë”©
print("ëª¨ë¸ ë¡œë”© ì¤‘...")
embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
sentiment_analyzer = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# ë¬¸ì¥ ë¶„ë¦¬ (í•œê¸€/ì˜ë¬¸/í˜¼í•© ëª¨ë‘ ìì—°ìŠ¤ëŸ½ê²Œ)
def split_sentences(text):
    # í•œê¸€: ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ, ì¤„ë°”ê¿ˆ, ì˜ë¬¸: .!?\n, í•œê¸€: ã€‚ï¼ï¼Ÿ í¬í•¨
    # ë¬¸ì¥ ëì— ê³µë°±/ì¤„ë°”ê¿ˆì´ ì˜¬ ìˆ˜ ìˆìŒ
    sentence_endings = re.compile(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+|\n+')
    sentences = sentence_endings.split(text)
    return [s.strip() for s in sentences if s.strip()]

# ì–¸ì–´ ê°ì§€
def detect_language(text):
    korean_count = len(re.findall(r'[ê°€-í£]', text))
    english_count = len(re.findall(r'[a-zA-Z]', text))
    if korean_count > english_count and korean_count > 10:
        return 'Korean'
    elif english_count > korean_count and english_count > 10:
        return 'English'
    else:
        return 'Mixed'

# í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ TF-IDF)
def extract_keywords(sentences, top_n=10):
    words = []
    for s in sentences:
        words += re.findall(r'\b\w+\b', s.lower())
    word_counts = Counter(words)
    total_words = sum(word_counts.values())
    tf_idf_scores = {}
    for word, count in word_counts.items():
        tf = count / total_words
        idf = math.log(len(sentences) / (1 + sum(1 for s in sentences if word in s.lower())))
        tf_idf_scores[word] = tf * idf
    sorted_keywords = sorted(tf_idf_scores.items(), key=lambda x: x[1], reverse=True)
    return sorted_keywords[:top_n]

# MMR ê¸°ë°˜ ìš”ì•½ (í•µì‹¬ì„± + ë‹¤ì–‘ì„±)
def mmr(doc_embedding, sentence_embeddings, sentences, top_n=5, diversity=0.7):
    selected = []
    selected_idx = []
    candidate_idx = [i for i in range(len(sentences))]

    while len(selected) < top_n and candidate_idx:
        if not selected:
            # ì²« ë¬¸ì¥: ê°€ì¥ ì¤‘ìš”í•œ ê²ƒ ì„ íƒ (ë¬¸ì„œ ì¤‘ì‹¬ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒ)
            sim_to_doc = cosine_similarity(sentence_embeddings, [doc_embedding]).reshape(-1)
            first_idx = int(np.argmax(sim_to_doc))
            selected.append(sentences[first_idx])
            selected_idx.append(first_idx)
            candidate_idx.remove(first_idx)
        else:
            mmr_scores = []
            for idx in candidate_idx:
                sim_to_doc = cosine_similarity([sentence_embeddings[idx]], [doc_embedding])[0][0]
                sim_to_selected = max(cosine_similarity([sentence_embeddings[idx]], [sentence_embeddings[s]])[0][0] for s in selected_idx)
                score = diversity * sim_to_doc - (1 - diversity) * sim_to_selected
                mmr_scores.append((score, idx))
            mmr_scores.sort(reverse=True)
            best_idx = mmr_scores[0][1]
            selected.append(sentences[best_idx])
            selected_idx.append(best_idx)
            candidate_idx.remove(best_idx)

    # ì›ë¬¸ ìˆœì„œ ìœ ì§€
    ordered = sorted(zip(selected_idx, selected), key=lambda x: x[0])
    return [s for (_, s) in ordered]

# ìš”ì•½ ìƒì„±
def generate_summary(text: str, length_option: str = "normal") -> Tuple[str, List[str]]:
    sentences = split_sentences(text)
    if len(sentences) < 3:
        raise ValueError("ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 3ë¬¸ì¥ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
    
    embeddings = embedding_model.encode(sentences)
    if length_option == "short":
        n_clusters = min(3, len(sentences))
    elif length_option == "long":
        n_clusters = min(8, len(sentences))
    else:  # normal
        n_clusters = min(5, len(sentences))
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans.fit(embeddings)
    
    summary_sentences = []
    for i in range(n_clusters):
        cluster_indices = np.where(kmeans.labels_ == i)[0]
        cluster_embeddings = embeddings[cluster_indices]
        center = kmeans.cluster_centers_[i]
        distances = np.linalg.norm(cluster_embeddings - center, axis=1)
        closest_idx = cluster_indices[np.argmin(distances)]
        summary_sentences.append(sentences[closest_idx])

    summary_sentences.sort(key=lambda x: sentences.index(x))
    summary = ' '.join(summary_sentences)
    return summary, sentences

# ê°œì„ ëœ í‚¤ì›Œë“œ ê°•ì¡° (í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ ê²½ê³„)
def highlight_keywords(text, keywords):
    for keyword, _ in keywords:
        if not keyword.strip():
            continue  # ë¹ˆ í‚¤ì›Œë“œ ë¬´ì‹œ
        # í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ ê²½ê³„ì—ë§Œ ê°•ì¡°
        pattern = re.compile(rf'(?<![\wê°€-í£]){re.escape(keyword)}(?![\wê°€-í£])', re.IGNORECASE)
        text = pattern.sub(f"**{keyword}**", text)
    return text

# ê°ì • ë¶„ì„
def analyze_sentiment(text):
    result = sentiment_analyzer(text)
    label = result[0]['label']
    score = result[0]['score']
    return label, score

# ì „ì²´ íŒŒì´í”„ë¼ì¸
def summarize_system(text: str, length_option: str = "normal", highlight: bool = True):
    if not text or len(text) < 30:
        return "âš ï¸ ì…ë ¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œí•œ 2~3ë¬¸ì¥ ì´ìƒì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    try:
        sentences = split_sentences(text)
        if len(sentences) < 3:
            raise ValueError("ë¬¸ì¥ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")

        language = detect_language(text)

        sentence_embeddings = embedding_model.encode(sentences)
        doc_embedding = np.mean(sentence_embeddings, axis=0)

        # ìš”ì•½ ê¸¸ì´ ì„¤ì •
        if length_option == "short":
            n_sentences = min(3, len(sentences))
        elif length_option == "long":
            n_sentences = min(8, len(sentences))
        else:  # normal
            n_sentences = min(5, len(sentences))

        summary_sentences = mmr(doc_embedding, sentence_embeddings, sentences, top_n=n_sentences)
        summary = ' '.join(summary_sentences)

        keywords = extract_keywords(sentences, top_n=10)
        sentiment_label, sentiment_score = analyze_sentiment(summary)

        if highlight:
            summary_highlighted = highlight_keywords(summary, keywords)
        else:
            summary_highlighted = summary

        return {
            "summary": summary_highlighted,
            "keywords": keywords,
            "sentiment": (sentiment_label, sentiment_score),
            "original_length": len(text),
            "summary_length": len(summary),
            "sentence_count": len(sentences),
            "selected_sentences": len(summary_sentences),
            "detected_language": language
        }
    except Exception as e:
        print("ğŸš« ì˜¤ë¥˜ ë°œìƒ:", e)
        return None

def format_summary_output(summary_result: Dict) -> str:
    output = []
    output.append("ğŸ“ ìš”ì•½ ê²°ê³¼:")
    output.append("")
    output.append(summary_result["summary"])
    output.append("")
    if summary_result["keywords"]:
        output.append("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ:")
        for i, (kw, score) in enumerate(summary_result["keywords"], 1):
            output.append(f"  {i}. {kw} (TF-IDF: {score:.3f})")
        output.append("")
    output.append(f"ğŸŒ ì–¸ì–´ ê°ì§€: {summary_result['detected_language']}")
    label, score = summary_result["sentiment"]
    output.append(f"ğŸ˜Š ê°ì • ë¶„ì„: {label} (ì‹ ë¢°ë„: {score:.2f})")
    output.append("")
    output.append("ğŸ“Š í†µê³„:")
    output.append(f"  â€¢ ì›ë³¸ ê¸¸ì´: {summary_result['original_length']:,}ì")
    output.append(f"  â€¢ ìš”ì•½ ê¸¸ì´: {summary_result['summary_length']:,}ì")
    output.append(f"  â€¢ ë¬¸ì¥ ìˆ˜: {summary_result['sentence_count']}ê°œ â†’ {summary_result['selected_sentences']}ê°œ")
    return '\n'.join(output) 