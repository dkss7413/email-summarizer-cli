# ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ 

import re
import math
from typing import List, Tuple, Dict
from collections import Counter

import numpy as np
import torch
from transformers import (
    pipeline, PreTrainedTokenizerFast, BartForConditionalGeneration
)

# ---------------------------
# í™˜ê²½ ì„¤ì • (GPU ìë™ ê°ì§€)
# ---------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---------------------------
# ì–¸ì–´ ê°ì§€
# ---------------------------
# ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ í•œê¸€/ì˜ë¬¸ ë¹„ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
def detect_language(text):
    korean_count = len(re.findall(r'[ê°€-í£]', text))
    english_count = len(re.findall(r'[a-zA-Z]', text))
    if korean_count > english_count and korean_count > 10:
        return 'Korean'
    elif english_count > korean_count and english_count > 10:
        return 'English'
    else:
        return 'Mixed'

# ---------------------------
# ë¬¸ì¥ ë¶„ë¦¬
# ---------------------------
# ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
def split_sentences(text):
    sentence_endings = re.compile(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+|\n+')
    return [s.strip() for s in sentence_endings.split(text) if s.strip()]

# ---------------------------
# í‚¤ì›Œë“œ ì¶”ì¶œ
# ---------------------------
# ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¶ˆìš©ì–´ë¥¼ ì œì™¸í•œ ì£¼ìš” ë‹¨ì–´(í‚¤ì›Œë“œ)ë¥¼ TF-IDF ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
def extract_keywords(sentences, top_n=10):
    korean_stopwords = {'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ë³´ë‹¤', 'ìƒê°í•˜ë‹¤', 'ê²ƒ', 'ìˆ˜', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì—', 'ì˜', 'ë¡œ', 'ê³¼', 'ë„'}
    english_stopwords = {'the', 'and', 'is', 'are', 'to', 'in', 'that', 'it', 'with', 'as', 'for', 'on', 'was', 'this'}

    words = []
    for s in sentences:
        korean_words = re.findall(r'[ê°€-í£]{2,}', s)
        english_words = re.findall(r'\b[a-zA-Z]{3,}\b', s.lower())
        korean_words = [w for w in korean_words if w not in korean_stopwords]
        english_words = [w for w in english_words if w not in english_stopwords]
        words.extend(korean_words + english_words)

    if not words:
        return []
    word_counts = Counter(words)
    total_words = sum(word_counts.values())
    tf_idf_scores = {
        word: (count / total_words) * math.log(len(sentences) / (1 + sum(1 for s in sentences if word in s)))
        for word, count in word_counts.items()
    }
    return sorted(tf_idf_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]

# ---------------------------
# ê°ì • ë¶„ì„ (BERT ê¸°ë°˜)
# ---------------------------
# ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ê°ì •(ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë“±)ì„ ë¶„ì„í•©ë‹ˆë‹¤.
sentiment_analyzer = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

def analyze_sentiment(text):
    # pipelineì—ì„œ ì‚¬ìš©í•˜ëŠ” tokenizer ì§ì ‘ ë¶ˆëŸ¬ì˜¤ê¸°
    tokenizer = sentiment_analyzer.tokenizer
    max_tokens = 512
    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)
    # í† í°ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)
    result = sentiment_analyzer(truncated_text)
    label = result[0]['label']
    score = result[0]['score']
    return label, score

# ê°ì • ë¶„ì„ ê²°ê³¼(ì˜ë¬¸ ë¼ë²¨)ë¥¼ í•œê¸€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
def convert_sentiment_to_korean(label, score):
    sentiment_map = {
        "1 star": "ë§¤ìš° ë¶€ì •ì ", "2 stars": "ë¶€ì •ì ",
        "3 stars": "ì¤‘ë¦½ì ", "4 stars": "ê¸ì •ì ", "5 stars": "ë§¤ìš° ê¸ì •ì "
    }
    korean_sentiment = sentiment_map.get(label, label)
    confidence_level = "ë†’ìŒ" if score >= 0.6 else "ë³´í†µ" if score >= 0.3 else "ë‚®ìŒ"
    return korean_sentiment, confidence_level

# ---------------------------
# ìš”ì•½ ìˆ˜í–‰ (seq2seq)
# ---------------------------
# ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ì–¸ì–´ì— ë”°ë¼ BART/KoBART ëª¨ë¸ë¡œ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
def summarize_with_seq2seq(text: str, language: str, max_length=150, min_length=40) -> str:
    if language == "Korean":
        tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')
        model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization').to(device)

        input_ids = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True).to(device)
        summary_ids = model.generate(
            input_ids,
            max_length=max_length,
            min_length=min_length,
            length_penalty=2.0,
            num_beams=4,
            early_stopping=True
        )
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    elif language == "English":
        summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=0 if torch.cuda.is_available() else -1)
        result = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
        summary = result[0]['summary_text']

    else:
        summary = "âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì–¸ì–´ì…ë‹ˆë‹¤. í•œêµ­ì–´ë‚˜ ì˜ì–´ë¡œ ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    return summary

# ---------------------------
# í†µí•© íŒŒì´í”„ë¼ì¸
# ---------------------------
# í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì–¸ì–´ ê°ì§€, ìš”ì•½, ê°ì • ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œê¹Œì§€ í•œ ë²ˆì— ì²˜ë¦¬í•©ë‹ˆë‹¤.
def summarize_system_seq2seq(text: str, max_length: int = None, min_length: int = None) -> Dict:
    if not text or len(text) < 30:
        return {"error": "âš ï¸ ì…ë ¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œí•œ 2~3ë¬¸ì¥ ì´ìƒì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."}

    # ìë™ ê¸¸ì´ ê²°ì • ë¡œì§
    if max_length is None or min_length is None:
        num_chars = len(text)
        num_sentences = len(split_sentences(text))
        # ê¸´ ë‰´ìŠ¤(2000ì ì´ìƒ)ëŠ” ë” ê¸¸ê²Œ ìš”ì•½
        if num_chars >= 2000:
            auto_max = 250
            auto_min = 100
        elif num_chars < 300 or num_sentences <= 3:
            auto_max = 40
            auto_min = 15
        elif num_chars < 1000 or num_sentences <= 8:
            auto_max = 80
            auto_min = 30
        else:
            auto_max = 150
            auto_min = 50
        if max_length is None:
            max_length = auto_max
        if min_length is None:
            min_length = auto_min

    try:
        language = detect_language(text)
        summary = summarize_with_seq2seq(text, language, max_length=max_length, min_length=min_length)
        # ìš”ì•½ ê²°ê³¼ê°€ 1ë¬¸ì¥ ì´í•˜ì¼ ê²½ìš° min_lengthë¥¼ ëŠ˜ë ¤ì„œ í•œ ë²ˆ ë” ì‹œë„
        summary_sentences = split_sentences(summary)
        if len(summary_sentences) <= 1 and min_length < 120:
            summary = summarize_with_seq2seq(text, language, max_length=max_length, min_length=120)
            summary_sentences = split_sentences(summary)
        # ê°ì • ë¶„ì„: ì›ë¬¸ ì „ì²´ì™€ ìš”ì•½ë¬¸ ëª¨ë‘
        sentiment_label_full, sentiment_score_full = analyze_sentiment(text)
        sentiment_label_sum, sentiment_score_sum = analyze_sentiment(summary)
        keywords = extract_keywords(split_sentences(text), top_n=10)

        return {
            "summary": summary,
            "keywords": keywords,
            "sentiment_full": (sentiment_label_full, sentiment_score_full),
            "sentiment_summary": (sentiment_label_sum, sentiment_score_sum),
            "original_length": len(text),
            "summary_length": len(summary),
            "detected_language": language,
            "summary_sentence_count": len(summary_sentences)
        }

    except Exception as e:
        return {"error": f"ğŸš« ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

# ---------------------------
# ê²°ê³¼ ì¶œë ¥
# ---------------------------
# ìš”ì•½ ê²°ê³¼(ë”•ì…”ë„ˆë¦¬)ë¥¼ ë³´ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
def format_seq2seq_summary(summary_result: Dict) -> str:
    if "error" in summary_result:
        return summary_result["error"]

    output = []
    output.append("ğŸ“ ë¬¸ë§¥ ê¸°ë°˜ ìš”ì•½ ê²°ê³¼:")
    output.append("")
    output.append(summary_result["summary"])
    output.append("")

    output.append(f"ğŸŒ ì–¸ì–´ ê°ì§€: {summary_result['detected_language']}")
    # ê°ì • ë¶„ì„(ìš”ì•½) â†’ ê°ì • ë¶„ì„ ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€ê²½
    label_sum, score_sum = summary_result["sentiment_summary"]
    korean_sentiment_sum, confidence_level_sum = convert_sentiment_to_korean(label_sum, score_sum)
    output.append(f"ğŸ˜Š ê°ì • ë¶„ì„: {korean_sentiment_sum} (ì‹ ë¢°ë„: {confidence_level_sum})")
    output.append("")
    output.append("ğŸ“Š í†µê³„:")
    output.append(f"  â€¢ ì›ë³¸ ê¸¸ì´: {summary_result['original_length']:,}ì")
    output.append(f"  â€¢ ìš”ì•½ ê¸¸ì´: {summary_result['summary_length']:,}ì")
    output.append(f"  â€¢ ìš”ì•½ ë¬¸ì¥ ìˆ˜: {summary_result['summary_sentence_count']}ë¬¸ì¥")

    if summary_result["keywords"]:
        output.append("")
        output.append("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ:")
        output.append(", ".join([kw for kw, _ in summary_result["keywords"]]))

    return '\n'.join(output)

# ë¬¸ì„œ ìœ í˜• ê°ì§€
def detect_text_type(text: str) -> str:
    lines = text.strip().split('\n')
    if any(re.search(r'(ë³´ë‚¸[ ]?ì‚¬ëŒ|ë°›ëŠ”[ ]?ì‚¬ëŒ|ì œëª©|from|to|subject):', line.lower()) for line in lines[:5]):
        return 'email'
    return 'general'

# ìœ í˜•ë³„ ìš”ì•½ ì „ëµ
def get_summary_strategy(text_type: str) -> Dict:
    strategy = {
        'email': {
            'top_n': 4,
            'diversity': 0.6,
            'redundancy_threshold': 0.8,
            'preprocess': True
        },
        'general': {
            'top_n': 5,
            'diversity': 0.7,
            'redundancy_threshold': 0.85,
            'preprocess': False
        }
    }
    return strategy.get(text_type, strategy['general'])

# MMR ìš”ì•½
def mmr(doc_embedding, sentence_embeddings, sentences, top_n=5, diversity=0.7, redundancy_threshold=0.85):
    selected = []
    selected_idx = []
    candidate_idx = list(range(len(sentences)))

    while len(selected) < top_n and candidate_idx:
        if not selected:
            sim_to_doc = cosine_similarity(sentence_embeddings, [doc_embedding]).reshape(-1)
            first_idx = int(np.argmax(sim_to_doc))
            selected.append(sentences[first_idx])
            selected_idx.append(first_idx)
            candidate_idx.remove(first_idx)
        else:
            mmr_scores = []
            for idx in candidate_idx:
                sim_to_doc = cosine_similarity([sentence_embeddings[idx]], [doc_embedding])[0][0]
                sim_to_selected = max(
                    cosine_similarity([sentence_embeddings[idx]], [sentence_embeddings[s]])[0][0]
                    for s in selected_idx
                )
                if sim_to_selected > redundancy_threshold:
                    continue
                score = diversity * sim_to_doc - (1 - diversity) * sim_to_selected
                mmr_scores.append((score, idx))

            if not mmr_scores:
                break
            mmr_scores.sort(reverse=True)
            best_idx = mmr_scores[0][1]
            selected.append(sentences[best_idx])
            selected_idx.append(best_idx)
            candidate_idx.remove(best_idx)

    ordered = sorted(zip(selected_idx, selected), key=lambda x: x[0])
    return [s for _, s in ordered]

# í‚¤ì›Œë“œ ê°•ì¡°
def highlight_keywords(text, keywords):
    highlighted_text = text
    for keyword, _ in keywords:
        if not keyword.strip():
            continue
        pattern = re.compile(rf'(?<![\wê°€-í£]){re.escape(keyword)}(?![\wê°€-í£])', re.IGNORECASE)
        highlighted_text = pattern.sub(f"\033[1;36m{keyword}\033[0m", highlighted_text)
    return highlighted_text

# ìš”ì•½ íŒŒì´í”„ë¼ì¸
def summarize_system(text: str, highlight: bool = True):
    if not text or len(text) < 30:
        return "âš ï¸ ì…ë ¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œí•œ 2~3ë¬¸ì¥ ì´ìƒì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    try:
        text_type = detect_text_type(text)
        strategy = get_summary_strategy(text_type)

        sentences = split_sentences(text)
        if len(sentences) < 3:
            raise ValueError("ë¬¸ì¥ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")

        language = detect_language(text)
        sentence_embeddings = embedding_model.encode(sentences)
        doc_embedding = np.mean(sentence_embeddings, axis=0)

        summary_sentences = mmr(
            doc_embedding=doc_embedding,
            sentence_embeddings=sentence_embeddings,
            sentences=sentences,
            top_n=strategy['top_n'],
            diversity=strategy['diversity'],
            redundancy_threshold=strategy['redundancy_threshold']
        )
        summary = ' '.join(summary_sentences)

        keywords = extract_keywords(sentences, top_n=10)
        sentiment_label, sentiment_score = analyze_sentiment(summary)
        summary_highlighted = highlight_keywords(summary, keywords) if highlight else summary

        return {
            "summary": summary_highlighted,
            "keywords": keywords,
            "sentiment": (sentiment_label, sentiment_score),
            "original_length": len(text),
            "summary_length": len(summary),
            "sentence_count": len(sentences),
            "selected_sentences": len(summary_sentences),
            "detected_language": language,
            "text_type": text_type
        }

    except Exception as e:
        print("ğŸš« ì˜¤ë¥˜ ë°œìƒ:", e)
        return None

# ì¶œë ¥ í¬ë§·
def format_summary_output(summary_result: Dict) -> str:
    output = []
    output.append("ğŸ“ ìš”ì•½ ê²°ê³¼:")
    output.append("")

    summary_text = summary_result["summary"]
    lines = []

    if summary_result.get("text_type") == "email":
        header_patterns = [
            r'(ë³´ë‚´ëŠ” ì‚¬ëŒ\s*:\s*[^\s]+@[^\s]+)',
            r'(ë°›ëŠ” ì‚¬ëŒ\s*:\s*[^\s]+@[^\s]+)',
            r'(ì°¸ì¡°\s*:\s*[^\s]+@[^\s]+)',
            r'(ì œëª©\s*:\s*.+)'
        ]
        greeting_pattern = r'(ì•ˆë…•í•˜ì„¸ìš”[.!?\s]*)'
        rest = summary_text
        for pat in header_patterns:
            m = re.search(pat, rest)
            if m:
                lines.append(m.group(1).strip())
                rest = rest.replace(m.group(1), '').strip()
        m = re.search(greeting_pattern, rest)
        if m:
            lines.append(m.group(1).strip())
            rest = rest.replace(m.group(1), '').strip()
        sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+', rest)
        lines += [s.strip() for s in sentences if s.strip()]
    else:
        lines = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+', summary_text)

    output.append('\n'.join(lines))
    output.append("")
    output.append(f"ğŸ“„ ë¬¸ì„œ ìœ í˜•: {summary_result['text_type'].capitalize()}")
    output.append(f"ğŸŒ ì–¸ì–´ ê°ì§€: {summary_result['detected_language']}")
    label, score = summary_result["sentiment"]
    korean_sentiment, confidence_level = convert_sentiment_to_korean(label, score)
    output.append(f"ğŸ˜Š ê°ì • ë¶„ì„: {korean_sentiment} (ì‹ ë¢°ë„: {confidence_level})")
    output.append("")
    output.append("ğŸ“Š í†µê³„:")
    output.append(f"  â€¢ ì›ë³¸ ê¸¸ì´: {summary_result['original_length']:,}ì")
    output.append(f"  â€¢ ìš”ì•½ ê¸¸ì´: {summary_result['summary_length']:,}ì")
    output.append(f"  â€¢ ë¬¸ì¥ ìˆ˜: {summary_result['sentence_count']}ê°œ â†’ {summary_result['selected_sentences']}ê°œ")
    return '\n'.join(output) 