# ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ 

import re
from typing import List, Tuple, Dict
from collections import Counter
import math


def detect_language(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    
    Args:
        text: ì–¸ì–´ë¥¼ ê°ì§€í•  í…ìŠ¤íŠ¸
        
    Returns:
        str: ê°ì§€ëœ ì–¸ì–´ ('ko', 'en', 'mixed')
    """
    # í•œê¸€ ë¬¸ì íŒ¨í„´
    korean_pattern = r'[ê°€-í£]'
    # ì˜ë¬¸ ë¬¸ì íŒ¨í„´
    english_pattern = r'[a-zA-Z]'
    
    korean_count = len(re.findall(korean_pattern, text))
    english_count = len(re.findall(english_pattern, text))
    
    total_chars = len(re.findall(r'[ê°€-í£a-zA-Z]', text))
    
    if total_chars == 0:
        return 'ko'  # ê¸°ë³¸ê°’
    
    korean_ratio = korean_count / total_chars
    english_ratio = english_count / total_chars
    
    if korean_ratio > 0.7:
        return 'ko'
    elif english_ratio > 0.7:
        return 'en'
    else:
        return 'mixed'


def get_stopwords(language: str) -> set:
    """
    ì–¸ì–´ë³„ ë¶ˆìš©ì–´ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        language: ì–¸ì–´ ('ko', 'en', 'mixed')
        
    Returns:
        set: ë¶ˆìš©ì–´ ì§‘í•©
    """
    korean_stopwords = {
        # ê¸°ë³¸ ë¶ˆìš©ì–´
        'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ê³³', 'ë§', 'ì¼', 'ë…„', 'ì›”', 'ì¼',
        'ì‹œ', 'ë¶„', 'ì´ˆ', 'ê°œ', 'ëª…', 'ë²ˆ', 'íšŒ', 'ì°¨', 'ëŒ€', 'ë§ˆë¦¬', 'ê¶Œ', 'ì±„',
        'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””ì„œ',
        'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë©´',
        'ì´ì œ', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ì–´ë””',
        
        # ì¡°ì‚¬
        'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼',
        'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ê°™ì´', 'ë§ˆë‹¤', 'ë§ˆë‹¤', 'ë§ˆë‹¤',
        
        # ì ‘ì†ì‚¬
        'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë©´',
        'ë§Œì•½', 'ë§Œì¼', 'ë§Œì•½ì—', 'ë§Œì¼', 'ë§Œì•½ì—', 'ë§Œì¼', 'ë§Œì•½ì—', 'ë§Œì¼',
        
        # ë¶€ì‚¬
        'ë§¤ìš°', 'ë„ˆë¬´', 'ì•„ì£¼', 'ì •ë§', 'ì§„ì§œ', 'ì™„ì „', 'ì „í˜€', 'ì ˆëŒ€', 'ì•„ì§', 'ë²Œì¨',
        'ê³§', 'ë°”ë¡œ', 'ì¦‰ì‹œ', 'ë‹¹ì¥', 'ì§€ê¸ˆ', 'ì´ì œ', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ',
        
        # ëŒ€ëª…ì‚¬
        'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ì €í¬', 'ê·¸ë“¤', 'ê·¸ë…€', 'ê·¸', 'ì´', 'ì €', 'ëˆ„êµ¬', 'ë¬´ì—‡',
        'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ì–´ë””', 'ì–´ë–¤', 'ì–´ë–»ê²Œ',
        
        # ìˆ˜ì‚¬
        'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´',
        'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë„·ì§¸', 'ë‹¤ì„¯ì§¸', 'ì—¬ì„¯ì§¸', 'ì¼ê³±ì§¸', 'ì—¬ëŸì§¸', 'ì•„í™‰ì§¸', 'ì—´ì§¸',
        
        # ê¸°íƒ€
        'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤',
        'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤'
    }
    
    english_stopwords = {
        # ê¸°ë³¸ ë¶ˆìš©ì–´
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of',
        'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',
        'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
        'may', 'might', 'must', 'can', 'shall', 'ought', 'need', 'dare',
        
        # ì ‘ì†ì‚¬
        'and', 'or', 'but', 'nor', 'yet', 'so', 'although', 'because', 'since',
        'unless', 'while', 'where', 'when', 'if', 'then', 'else', 'though',
        
        # ì „ì¹˜ì‚¬
        'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about',
        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between',
        'among', 'within', 'without', 'against', 'toward', 'towards', 'upon',
        
        # ëŒ€ëª…ì‚¬
        'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
        'my', 'your', 'his', 'her', 'its', 'our', 'their', 'mine', 'yours', 'his',
        'hers', 'ours', 'theirs', 'this', 'that', 'these', 'those', 'who', 'whom',
        'whose', 'which', 'what', 'where', 'when', 'why', 'how',
        
        # ë¶€ì‚¬
        'very', 'too', 'so', 'quite', 'rather', 'really', 'just', 'only', 'even',
        'still', 'already', 'yet', 'now', 'then', 'here', 'there', 'where',
        'when', 'why', 'how', 'well', 'badly', 'quickly', 'slowly', 'easily',
        
        # ì¡°ë™ì‚¬
        'can', 'could', 'may', 'might', 'will', 'would', 'shall', 'should',
        'must', 'ought', 'need', 'dare', 'used',
        
        # ê¸°íƒ€
        'yes', 'no', 'not', 'n\'t', 'all', 'any', 'both', 'each', 'few', 'more',
        'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',
        'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',
        'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain',
        'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn',
        'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren',
        'won', 'wouldn'
    }
    
    if language == 'ko':
        return korean_stopwords
    elif language == 'en':
        return english_stopwords
    else:  # mixed
        return korean_stopwords.union(english_stopwords)


def extract_words(text: str, language: str = 'ko') -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì–¸ì–´ë³„ ìµœì í™”)
    
    Args:
        text: ë‹¨ì–´ë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
        language: ì–¸ì–´ ('ko', 'en', 'mixed')
        
    Returns:
        List[str]: ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    """
    # ì–¸ì–´ë³„ ë‹¨ì–´ ì¶”ì¶œ íŒ¨í„´
    if language == 'ko':
        # í•œê¸€, ì˜ë¬¸, ìˆ«ì ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text.lower())
    elif language == 'en':
        # ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ (ë” ì—„ê²©í•œ ì˜ë¬¸ ì²˜ë¦¬)
        words = re.findall(r'[a-zA-Z]+', text.lower())
    else:  # mixed
        # í•œê¸€, ì˜ë¬¸, ìˆ«ì ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text.lower())
    
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = get_stopwords(language)
    
    # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
    filtered_words = []
    for word in words:
        # ê¸¸ì´ í•„í„°ë§ (í•œê¸€: 2ì ì´ìƒ, ì˜ë¬¸: 3ì ì´ìƒ)
        min_length = 2 if language == 'ko' else 3
        if len(word) >= min_length and word not in stopwords:
            filtered_words.append(word)
    
    return filtered_words


def split_sentences(text: str, language: str = 'ko') -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. (ì–¸ì–´ë³„ ìµœì í™”)
    
    Args:
        text: ë¶„ë¦¬í•  í…ìŠ¤íŠ¸
        language: ì–¸ì–´ ('ko', 'en', 'mixed')
        
    Returns:
        List[str]: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    """
    if language == 'ko':
        # í•œê¸€ ë¬¸ì¥ ë íŒ¨í„´
        sentence_pattern = r'[.!?]+'
    elif language == 'en':
        # ì˜ë¬¸ ë¬¸ì¥ ë íŒ¨í„´ (ë” ì •êµí•œ ì²˜ë¦¬)
        sentence_pattern = r'[.!?]+(?=\s|$)'
    else:  # mixed
        # í˜¼í•© ì–¸ì–´ ë¬¸ì¥ ë íŒ¨í„´
        sentence_pattern = r'[.!?]+'
    
    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = re.split(sentence_pattern, text)
    
    # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ê³µë°± ì •ë¦¬
    sentences = [s.strip() for s in sentences if s.strip()]
    
    return sentences


def calculate_tf_idf_keywords(text: str, top_n: int = 10, language: str = 'ko') -> List[Tuple[str, float]]:
    """
    TF-IDF ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì–¸ì–´ë³„ ìµœì í™”)
    
    Args:
        text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
        language: ì–¸ì–´ ('ko', 'en', 'mixed')
        
    Returns:
        List[Tuple[str, float]]: (í‚¤ì›Œë“œ, TF-IDF ì ìˆ˜) ë¦¬ìŠ¤íŠ¸
    """
    # ì–¸ì–´ ê°ì§€ (ìë™ ê°ì§€ê°€ ìš°ì„ )
    detected_lang = detect_language(text)
    if language == 'auto':
        language = detected_lang
    
    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = split_sentences(text, language)
    if not sentences:
        return []
    
    # ê° ë¬¸ì¥ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
    sentence_words = [extract_words(sentence, language) for sentence in sentences]
    
    # ì „ì²´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    all_words = extract_words(text, language)
    total_words = len(all_words)
    
    if total_words == 0:
        return []
    
    # ë‹¨ì–´ë³„ TF-IDF ì ìˆ˜ ê³„ì‚°
    word_scores = {}
    
    for word in set(all_words):
        # TF (Term Frequency) ê³„ì‚°
        tf = all_words.count(word) / total_words
        
        # IDF (Inverse Document Frequency) ê³„ì‚°
        sentences_with_word = sum(1 for words in sentence_words if word in words)
        idf = math.log(len(sentences) / (sentences_with_word + 1))
        
        # TF-IDF ì ìˆ˜
        tf_idf = tf * idf
        
        # ì–¸ì–´ë³„ ì¶”ê°€ ê°€ì¤‘ì¹˜
        bonus = 0.0
        
        if language == 'ko':
            # í•œê¸€ íŠ¹ë³„ íŒ¨í„´
            if re.search(r'ì£¼ìš”|í•µì‹¬|ì¤‘ìš”|íŠ¹ì§•|ê¸°ëŠ¥|ëª©ì |ì˜ë¯¸|ê°œë°œ|ì‹œìŠ¤í…œ|í”„ë¡œì íŠ¸', word):
                bonus += 0.5
            if re.search(r'ê°œì„ |ìµœì í™”|ì„±ëŠ¥|ë³´ì•ˆ|í…ŒìŠ¤íŠ¸|ë¶„ì„', word):
                bonus += 0.3
        elif language == 'en':
            # ì˜ë¬¸ íŠ¹ë³„ íŒ¨í„´
            if re.search(r'important|key|main|primary|essential|critical|major', word.lower()):
                bonus += 0.5
            if re.search(r'feature|function|system|project|development|analysis', word.lower()):
                bonus += 0.3
            if re.search(r'api|web|app|database|server|client|framework', word.lower()):
                bonus += 0.2
        else:  # mixed
            # í˜¼í•© ì–¸ì–´ íŒ¨í„´
            if re.search(r'ì£¼ìš”|í•µì‹¬|ì¤‘ìš”|íŠ¹ì§•|ê¸°ëŠ¥|ëª©ì |ì˜ë¯¸|ê°œë°œ|ì‹œìŠ¤í…œ|í”„ë¡œì íŠ¸|important|key|main|primary|essential', word.lower()):
                bonus += 0.5
            if re.search(r'api|web|app|database|server|client|framework', word.lower()):
                bonus += 0.3
        
        word_scores[word] = tf_idf + bonus
    
    # ì˜ì–´ì˜ ê²½ìš° ë‹¨ìˆ˜í˜•ê³¼ ë³µìˆ˜í˜• í†µí•©
    if language == 'en':
        # ë‹¨ìˆ˜í˜•ê³¼ ë³µìˆ˜í˜• ë§¤í•‘
        singular_forms = {}
        for word in list(word_scores.keys()):  # keys()ë¥¼ listë¡œ ë³µì‚¬
            if word.endswith('s') and len(word) > 3:
                # ë³µìˆ˜í˜•ì¸ ê²½ìš° ë‹¨ìˆ˜í˜• ì°¾ê¸°
                singular = word[:-1]  # s ì œê±°
                if singular in word_scores:
                    # ë‹¨ìˆ˜í˜•ê³¼ ë³µìˆ˜í˜• ì ìˆ˜ í•©ì‚°
                    combined_score = word_scores[word] + word_scores[singular]
                    singular_forms[singular] = combined_score
                    # ë³µìˆ˜í˜• ì œê±°
                    del word_scores[word]
                else:
                    # ë‹¨ìˆ˜í˜•ì´ ì—†ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ìœ ì§€
                    pass
            elif not word.endswith('s') and len(word) > 2:
                # ë‹¨ìˆ˜í˜•ì¸ ê²½ìš° ë³µìˆ˜í˜• í™•ì¸
                plural = word + 's'
                if plural in word_scores:
                    # ì´ë¯¸ ë³µìˆ˜í˜•ì—ì„œ ì²˜ë¦¬ë¨
                    pass
                else:
                    # ë‹¨ìˆ˜í˜•ë§Œ ìˆëŠ” ê²½ìš°
                    singular_forms[word] = word_scores[word]
        # í†µí•©ëœ ì ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        for word, score in singular_forms.items():
            word_scores[word] = score
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ Nê°œ ë°˜í™˜
    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)
    return sorted_words[:top_n]


def extract_keywords(text: str, top_n: int = 10) -> List[Tuple[str, int]]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ê¸°ì¡´ ë°©ì‹ - í˜¸í™˜ì„± ìœ ì§€)
    
    Args:
        text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
        
    Returns:
        List[Tuple[str, int]]: (í‚¤ì›Œë“œ, ë¹ˆë„ìˆ˜) ë¦¬ìŠ¤íŠ¸
    """
    language = detect_language(text)
    words = extract_words(text, language)
    word_counts = Counter(words)
    return word_counts.most_common(top_n)


def calculate_sentence_score(sentence: str, keywords: List[Tuple[str, float]], language: str = 'ko') -> float:
    """
    ë¬¸ì¥ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (ì–¸ì–´ë³„ ìµœì í™”)
    
    Args:
        sentence: ì ìˆ˜ë¥¼ ê³„ì‚°í•  ë¬¸ì¥
        keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (TF-IDF ì ìˆ˜ í¬í•¨)
        language: ì–¸ì–´ ('ko', 'en', 'mixed')
        
    Returns:
        float: ë¬¸ì¥ ì ìˆ˜
    """
    sentence_lower = sentence.lower()
    score = 0.0
    
    # í‚¤ì›Œë“œ í¬í•¨ ì ìˆ˜ (TF-IDF ì ìˆ˜ ë°˜ì˜)
    for keyword, tf_idf_score in keywords:
        if keyword in sentence_lower:
            score += tf_idf_score * 2  # TF-IDF ì ìˆ˜ì— ê°€ì¤‘ì¹˜
    
    # ë¬¸ì¥ ê¸¸ì´ ì ìˆ˜ (ì–¸ì–´ë³„ ìµœì  ê¸¸ì´)
    length = len(sentence)
    if language == 'ko':
        if 15 <= length <= 120:
            score += 1.5
        elif 10 <= length < 15 or 120 < length <= 200:
            score += 1.0
        elif length < 10:
            score += 0.3
    elif language == 'en':
        if 20 <= length <= 150:
            score += 1.5
        elif 15 <= length < 20 or 150 < length <= 250:
            score += 1.0
        elif length < 15:
            score += 0.3
    else:  # mixed
        if 15 <= length <= 120:
            score += 1.5
        elif 10 <= length < 15 or 120 < length <= 200:
            score += 1.0
        elif length < 10:
            score += 0.3
    
    # ì–¸ì–´ë³„ íŠ¹ë³„í•œ íŒ¨í„´ ì ìˆ˜
    if language == 'ko':
        important_patterns = [
            (r'ì£¼ìš”|í•µì‹¬|ì¤‘ìš”|íŠ¹ì§•|ê¸°ëŠ¥|ëª©ì |ì˜ë¯¸', 3.0),
            (r'ì˜ˆì‹œ|ì˜ˆë¥¼|ì˜ˆì‹œë¡œ|êµ¬ì²´ì ìœ¼ë¡œ', 2.0),
            (r'ê°œë°œ|êµ¬í˜„|ì„¤ê³„|ì•„í‚¤í…ì²˜', 2.5),
            (r'ìµœì í™”|ì„±ëŠ¥|ë³´ì•ˆ|í…ŒìŠ¤íŠ¸', 2.0),
            (r'ê²°ë¡ |ìš”ì•½|ì •ë¦¬', 1.5)
        ]
    elif language == 'en':
        important_patterns = [
            (r'important|key|main|primary|essential|critical', 3.0),
            (r'example|instance|specifically|concretely', 2.0),
            (r'develop|implement|design|architecture', 2.5),
            (r'optimize|performance|security|test', 2.0),
            (r'conclusion|summary|conclude', 1.5)
        ]
    else:  # mixed
        important_patterns = [
            (r'ì£¼ìš”|í•µì‹¬|ì¤‘ìš”|íŠ¹ì§•|ê¸°ëŠ¥|ëª©ì |ì˜ë¯¸|important|key|main|primary|essential', 3.0),
            (r'ì˜ˆì‹œ|ì˜ˆë¥¼|ì˜ˆì‹œë¡œ|êµ¬ì²´ì ìœ¼ë¡œ|example|instance|specifically', 2.0),
            (r'ê°œë°œ|êµ¬í˜„|ì„¤ê³„|ì•„í‚¤í…ì²˜|develop|implement|design|architecture', 2.5),
            (r'ìµœì í™”|ì„±ëŠ¥|ë³´ì•ˆ|í…ŒìŠ¤íŠ¸|optimize|performance|security|test', 2.0),
            (r'ê²°ë¡ |ìš”ì•½|ì •ë¦¬|conclusion|summary|conclude', 1.5)
        ]
    
    for pattern, pattern_score in important_patterns:
        if re.search(pattern, sentence, re.IGNORECASE):
            score += pattern_score
    
    # ìœ„ì¹˜ ì ìˆ˜ (ë¬¸ì¥ì˜ ìœ„ì¹˜ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜)
    sentences = split_sentences(sentence, language)
    if sentences:
        sentence_index = sentences.index(sentence) if sentence in sentences else 0
        total_sentences = len(sentences)
        
        # ì²« ë²ˆì§¸ ë¬¸ì¥ê³¼ ë§ˆì§€ë§‰ ë¬¸ì¥ì— ê°€ì¤‘ì¹˜
        if sentence_index == 0:
            score += 1.0
        elif sentence_index == total_sentences - 1:
            score += 0.8
        elif sentence_index < total_sentences * 0.3:  # ì•ë¶€ë¶„
            score += 0.5
    
    return score


def summarize_text(text: str, length: str = "short", language: str = "auto") -> Dict:
    """
    í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤. (ì–¸ì–´ë³„ ìµœì í™”)
    
    Args:
        text: ìš”ì•½í•  í…ìŠ¤íŠ¸
        length: ìš”ì•½ ê¸¸ì´ ("short" ë˜ëŠ” "long")
        language: ì–¸ì–´ ("ko", "en", "mixed", "auto")
        
    Returns:
        Dict: ìš”ì•½ ê²°ê³¼
    """
    # ì–¸ì–´ ê°ì§€
    detected_language = detect_language(text)
    if language == "auto":
        language = detected_language
    
    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = split_sentences(text, language)
    
    if not sentences:
        return {
            "summary": "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.",
            "keywords": [],
            "original_length": len(text),
            "summary_length": 0,
            "detected_language": language
        }
    
    # TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords_tfidf = calculate_tf_idf_keywords(text, language=language)
    
    # ë¬¸ì¥ ì ìˆ˜ ê³„ì‚°
    sentence_scores = []
    for sentence in sentences:
        score = calculate_sentence_score(sentence, keywords_tfidf, language)
        sentence_scores.append((sentence, score))
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    sentence_scores.sort(key=lambda x: x[1], reverse=True)
    
    # ìš”ì•½ ê¸¸ì´ ê²°ì •
    if length == "short":
        summary_count = min(3, len(sentences))
    else:  # long
        summary_count = min(5, len(sentences))
    
    # ìƒìœ„ ë¬¸ì¥ë“¤ ì„ íƒ
    selected_sentences = sentence_scores[:summary_count]
    
    # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    selected_sentences.sort(key=lambda x: sentences.index(x[0]))
    
    # ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
    summary_text = ". ".join([s[0] for s in selected_sentences]) + "."
    
    return {
        "summary": summary_text,
        "keywords": keywords_tfidf[:5],  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
        "original_length": len(text),
        "summary_length": len(summary_text),
        "sentence_count": len(sentences),
        "selected_sentences": len(selected_sentences),
        "detected_language": detected_language,
        "processing_language": language
    }


def format_summary_output(summary_result: Dict, highlight: bool = False) -> str:
    """
    ìš”ì•½ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤. (ì–¸ì–´ ì •ë³´ í¬í•¨)
    
    Args:
        summary_result: ìš”ì•½ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        highlight: í‚¤ì›Œë“œ ê°•ì¡° ì—¬ë¶€
        
    Returns:
        str: í¬ë§·íŒ…ëœ ìš”ì•½ ê²°ê³¼
    """
    output = []
    
    # ìš”ì•½ í…ìŠ¤íŠ¸
    output.append("ğŸ“ ìš”ì•½ ê²°ê³¼:")
    output.append("")
    
    if highlight and summary_result["keywords"]:
        summary_text = summary_result["summary"]
        detected_lang = summary_result.get("detected_language", "ko")
        
        # ê¸´ í‚¤ì›Œë“œë¶€í„° ì •ë ¬ (ì¤‘ì²© ë°©ì§€)
        sorted_keywords = sorted(summary_result["keywords"], key=lambda x: -len(x[0]))
        import re
        for keyword, score in sorted_keywords:
            if score > 1.0:
                highlight_style = f"\033[1;33m**{keyword}**\033[0m"
            else:
                highlight_style = f"\033[1;36m**{keyword}**\033[0m"
            if detected_lang == "en":
                # negative lookbehind: ê°•ì¡°ëœ ë¶€ë¶„(ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤) ë’¤ê°€ ì•„ë‹Œ ê³³ë§Œ ë§¤ì¹­
                # (?<!\033\[1;33m\*\*) ë“±ìœ¼ë¡œ ì´ë¯¸ ê°•ì¡°ëœ ë¶€ë¶„ ì œì™¸
                # ë‹¨, lookbehindëŠ” ê³ ì • ê¸¸ì´ë§Œ ì§€ì›í•˜ë¯€ë¡œ, ê°•ì¡° íŒ¨í„´ì„ ë‹¨ìˆœí™”
                pattern = re.compile(r'(?<!\*\*)(?<!\033\[1;33m\*\*)(?<!\033\[1;36m\*\*)' + re.escape(keyword) + r'(?!\*\*)(?!\033\[0m)', re.IGNORECASE)
                summary_text = pattern.sub(highlight_style, summary_text)
                if not keyword.endswith('s'):
                    plural_keyword = keyword + 's'
                    plural_style = f"\033[1;33m**{plural_keyword}**\033[0m" if score > 1.0 else f"\033[1;36m**{plural_keyword}**\033[0m"
                    pattern_plural = re.compile(r'(?<!\*\*)(?<!\033\[1;33m\*\*)(?<!\033\[1;36m\*\*)' + re.escape(plural_keyword) + r'(?!\*\*)(?!\033\[0m)', re.IGNORECASE)
                    summary_text = pattern_plural.sub(plural_style, summary_text)
            else:
                # í•œê¸€/í˜¼í•©: ê¸°ì¡´ ë°©ì‹, ë‹¨ ì´ë¯¸ ê°•ì¡°ëœ ë¶€ë¶„ì€ ê±´ë„ˆëœ€
                def replacer_ko(m):
                    if '\033[' in m.group(0):
                        return m.group(0)
                    return highlight_style
                pattern_ko = re.compile(re.escape(keyword))
                summary_text = pattern_ko.sub(replacer_ko, summary_text)
        output.append(summary_text)
    else:
        output.append(summary_result["summary"])
    
    output.append("")
    
    # í‚¤ì›Œë“œ (ì¤‘ìš”ë„ ìˆœ)
    if summary_result["keywords"]:
        output.append("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ì¤‘ìš”ë„ ìˆœ):")
        for i, (keyword, score) in enumerate(summary_result["keywords"], 1):
            if highlight:
                if score > 1.0:
                    output.append(f"  {i}. \033[1;33m**{keyword}**\033[0m (ì ìˆ˜: {score:.2f})")
                else:
                    output.append(f"  {i}. \033[1;36m**{keyword}**\033[0m (ì ìˆ˜: {score:.2f})")
            else:
                output.append(f"  {i}. {keyword} (ì ìˆ˜: {score:.2f})")
        output.append("")
    
    # ì–¸ì–´ ì •ë³´
    if "detected_language" in summary_result:
        lang_names = {"ko": "í•œêµ­ì–´", "en": "ì˜ì–´", "mixed": "í˜¼í•© ì–¸ì–´"}
        detected = lang_names.get(summary_result["detected_language"], summary_result["detected_language"])
        processing = lang_names.get(summary_result["processing_language"], summary_result["processing_language"])
        output.append(f"ğŸŒ ì–¸ì–´ ì •ë³´: ê°ì§€ë¨={detected}, ì²˜ë¦¬ë¨={processing}")
        output.append("")
    
    # í†µê³„ ì •ë³´
    output.append("ğŸ“Š ìš”ì•½ í†µê³„:")
    output.append(f"  â€¢ ì›ë³¸ ê¸¸ì´: {summary_result['original_length']:,}ì")
    output.append(f"  â€¢ ìš”ì•½ ê¸¸ì´: {summary_result['summary_length']:,}ì")
    output.append(f"  â€¢ ì••ì¶•ë¥ : {((1 - summary_result['summary_length'] / summary_result['original_length']) * 100):.1f}%")
    output.append(f"  â€¢ ë¬¸ì¥ ìˆ˜: {summary_result['sentence_count']}ê°œ â†’ {summary_result['selected_sentences']}ê°œ")
    
    return "\n".join(output) 